from dotenv import load_dotenv
from langchain.chat_models import init_chat_model #Buscar na documenta칞칚o do LangChain

import streamlit as st
import logging

load_dotenv()

logging.basicConfig(level=logging.INFO, format="{levelname}:{name}:{message}", style='{')
logger = logging.getLogger("chat-completion")

async def iterator2generator(async_iter):
    async for chunk in async_iter:
        yield chunk.content # Pausa, retorna o chunk.content e continua para o pr칩ximo chunk

st.set_page_config(page_icon='游눫')
st.title("Not a RAG Chat")

if model:= st.selectbox('Select a model:', ('GPT-4 Turbo', 'GPT-4.1 mini', 'GPT-4.1 nano')):
    try:
        chat = init_chat_model(f'openai:{model.lower().replace(' ', '-')}')

        # Respons치vel por preservar o hist칩rico da conversa mesmo com a 're-renderiza칞칚o' da p치gina
        if 'messages' not in st.session_state:
            st.session_state.messages = []  
        for message in st.session_state.messages:
            with st.chat_message(message['role']):
                st.markdown(message['content'])

        # L칩gica do Chat 
        if prompt:= st.chat_input('What is up?'): 
            with st.chat_message('user'):
                st.markdown(prompt)
                st.session_state.messages.append({'role': 'user', 'content': prompt})

            with st.chat_message('ai'):
                try: 
                    response_stream = chat.astream(prompt) # O m칠todo .asstream() retorna um objeto do tipo AsyncIterator
                    response = st.write_stream(iterator2generator(response_stream)) # .write_stream() espera um objeto do tipo AsyncGenerator ou uma fun칞칚o que yielde valores
                    st.session_state.messages.append({'role': 'assistant', 'content': response})
                except Exception as e:
                    logger.error(f'Erro na resposta da LLM: {e}')

    except Exception as e: 
        logger.error(f'Erro ao inicializar o modelo: {e}')
