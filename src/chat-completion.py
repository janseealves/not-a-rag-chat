from dotenv import load_dotenv
from langchain.chat_models import init_chat_model #Buscar na documenta칞칚o do LangChain

import streamlit as st
import logging

load_dotenv()

logging.basicConfig(level=logging.INFO, format='{levelname}:{name}:{message}', style='{')
logger = logging.getLogger('chat-completion')

st.set_page_config(page_icon='游눫')
st.title('Not a RAG Chat')

with st.expander('Setup 丘뙖잺'):
    model = st.selectbox('Model:', ('GPT-4 Turbo', 'GPT-4.1 mini', 'GPT-4.1 nano'))
    temperature = st.slider('Temperature:', 0.0, 1.0, 0.3, 0.1)
    memory_size = st.slider('Memory Size:', 0, 20, 10, 1)

if model:
    try:
        chat = init_chat_model(f'openai:{model.lower().replace(' ', '-')}', temperature=temperature)

        # Respons치vel por preservar o hist칩rico da conversa mesmo com a 're-renderiza칞칚o' da p치gina
        if 'messages' not in st.session_state:
            st.session_state.messages = []  
        for message in st.session_state.messages:
            with st.chat_message(message['role']):
                st.markdown(message['content'])

        # L칩gica do Chat 
        if prompt:= st.chat_input('What is up?'): 
            with st.chat_message('user'):
                st.markdown(prompt)
                st.session_state.messages.append({'role': 'user', 'content': prompt})

            with st.chat_message('ai'):
                try:
                    response_stream = chat.stream(st.session_state.messages)
                    response = st.write_stream(response_stream)
                    st.session_state.messages.append({'role': 'assistant', 'content': response})
                    logger.info(response)
                except Exception as e:
                    logger.error(f'Erro na resposta da LLM: {e}')

        # Clear messages history
        if len(st.session_state.messages) >= memory_size:
            import time
            
            with st.status('Memory limit exceeded, resetting cache...') as status:
                st.session_state.messages.clear()
                time.sleep(2)
                status.update(label= 'Memory lost!', state='complete')

    except Exception as e: 
        logger.error(f'Erro ao inicializar o modelo: {e}')
