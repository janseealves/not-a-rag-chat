{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02943bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Diret√≥rio de dados: C:\\Users\\User\\Workplace\\not-a-rag-chat\\src\\notebooks\\data\n",
      "üñºÔ∏è Diret√≥rio de plots por p√°gina: C:\\Users\\User\\Workplace\\not-a-rag-chat\\src\\notebooks\\outputs\\page_plots\n",
      "üß© Diret√≥rio de plots de chunks: C:\\Users\\User\\Workplace\\not-a-rag-chat\\src\\notebooks\\outputs\\chunk_plots\n",
      "üìÑ PDFs detectados: 9\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from langchain_unstructured import UnstructuredLoader\n",
    "\n",
    "# Descobrir diret√≥rios relevantes a partir do contexto atual\n",
    "_CWD = Path.cwd()\n",
    "_DATA_DIR_CANDIDATES = [\n",
    "    _CWD / \"data\",\n",
    "    _CWD / \"src/notebooks/data\"\n",
    "]\n",
    "\n",
    "for candidate in _DATA_DIR_CANDIDATES:\n",
    "    if candidate.exists():\n",
    "        DATA_DIR = candidate.resolve()\n",
    "        break\n",
    "else:\n",
    "    searched = \", \".join(str(c.resolve()) for c in _DATA_DIR_CANDIDATES)\n",
    "    raise FileNotFoundError(f\"N√£o foi poss√≠vel localizar o diret√≥rio de PDFs. Caminhos testados: {searched}\")\n",
    "\n",
    "OUTPUT_ROOT = DATA_DIR.parent / \"outputs\"\n",
    "ELEMENTS_OUTPUT_DIR = OUTPUT_ROOT / \"elements\"\n",
    "PAGE_PLOTS_DIR = OUTPUT_ROOT / \"page_plots\"\n",
    "CHUNK_PLOTS_DIR = OUTPUT_ROOT / \"chunk_plots\"\n",
    "\n",
    "for directory in (OUTPUT_ROOT, ELEMENTS_OUTPUT_DIR, PAGE_PLOTS_DIR, CHUNK_PLOTS_DIR):\n",
    "    directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PDF_PATHS = sorted({p.resolve() for pattern in (\"*.pdf\", \"*.PDF\") for p in DATA_DIR.glob(pattern)})\n",
    "\n",
    "if not PDF_PATHS:\n",
    "    raise FileNotFoundError(f\"Nenhum PDF encontrado em {DATA_DIR}\")\n",
    "\n",
    "print(f\"üìÅ Diret√≥rio de dados: {DATA_DIR}\")\n",
    "print(f\"üñºÔ∏è Diret√≥rio de plots por p√°gina: {PAGE_PLOTS_DIR}\")\n",
    "print(f\"üß© Diret√≥rio de plots de chunks: {CHUNK_PLOTS_DIR}\")\n",
    "print(f\"üìÑ PDFs detectados: {len(PDF_PATHS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af63666b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilit√°rio para serializar elementos carregados\n",
    "import json\n",
    "\n",
    "def _element_to_serializable(el):\n",
    "    # Prefer pydantic v2 model_dump, then v1 dict, then __dict__ fallback\n",
    "    if hasattr(el, \"model_dump\"):\n",
    "        data = el.model_dump()\n",
    "    elif hasattr(el, \"dict\"):\n",
    "        data = el.dict()\n",
    "    elif hasattr(el, \"__dict__\"):\n",
    "        data = el.__dict__\n",
    "    else:\n",
    "        return str(el)\n",
    "\n",
    "    def _convert(obj):\n",
    "        if isinstance(obj, dict):\n",
    "            return {k: _convert(v) for k, v in obj.items()}\n",
    "        if isinstance(obj, (list, tuple)):\n",
    "            return [_convert(v) for v in obj]\n",
    "        if isinstance(obj, (str, int, float, bool)) or obj is None:\n",
    "            return obj\n",
    "        try:\n",
    "            return [_convert(v) for v in obj]\n",
    "        except Exception:\n",
    "            return str(obj)\n",
    "\n",
    "    return _convert(data)\n",
    "\n",
    "def dump_elements(elements, destination_path):\n",
    "    destination_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(destination_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump([_element_to_serializable(element) for element in elements], f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a6e479a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from collections import Counter\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "def generate_category_colors(categories):\n",
    "    \"\"\"Gera cores distintas para cada categoria usando colormap.\"\"\"\n",
    "    if not categories:\n",
    "        return {}\n",
    "    n_categories = len(categories)\n",
    "    cmap = plt.cm.get_cmap('tab20' if n_categories <= 20 else 'hsv')\n",
    "    colors = {}\n",
    "    for idx, cat in enumerate(categories):\n",
    "        color_rgba = cmap(idx / max(n_categories - 1, 1))\n",
    "        colors[cat] = mcolors.rgb2hex(color_rgba[:3])\n",
    "    return colors\n",
    "\n",
    "def highlight_elements(pdf_path, elements, output_dir, render_scale=2):\n",
    "    pdf_output_dir = (output_dir / pdf_path.stem)\n",
    "    pdf_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    doc = fitz.open(pdf_path)\n",
    "    categories = sorted(set(elem.metadata.get('category', 'Unknown') for elem in elements))\n",
    "    category_colors = generate_category_colors(categories)\n",
    "    page_stats = []\n",
    "\n",
    "    try:\n",
    "        for page_index in range(doc.page_count):\n",
    "            page_num = page_index + 1\n",
    "            page_elements = [elem for elem in elements if elem.metadata.get('page_number') == page_num]\n",
    "            if not page_elements:\n",
    "                continue\n",
    "\n",
    "            page = doc[page_index]\n",
    "            pix = page.get_pixmap(matrix=fitz.Matrix(render_scale, render_scale))\n",
    "            img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(14, 18))\n",
    "            ax.imshow(img)\n",
    "            ax.axis('off')\n",
    "\n",
    "            for elem in page_elements:\n",
    "                coords = elem.metadata.get('coordinates')\n",
    "                category = elem.metadata.get('category', 'Unknown')\n",
    "                if coords and coords.get('points'):\n",
    "                    try:\n",
    "                        points = coords['points']\n",
    "                        x1, y1 = points[0]\n",
    "                        x2, y2 = points[2]\n",
    "                        x = x1 * render_scale\n",
    "                        y = y1 * render_scale\n",
    "                        width = (x2 - x1) * render_scale\n",
    "                        height = (y2 - y1) * render_scale\n",
    "                        color = category_colors.get(category, '#CCCCCC')\n",
    "                        rect = patches.Rectangle(\n",
    "                    (x, y), width, height,\n",
    "                    linewidth=0,\n",
    "                    edgecolor='none',\n",
    "                    facecolor=color,\n",
    "                    alpha=0.4\n",
    "                )\n",
    "                        ax.add_patch(rect)\n",
    "                    except Exception:\n",
    "                        continue\n",
    "\n",
    "            legend_elements = [patches.Patch(facecolor=color, label=cat, alpha=0.6) for cat, color in category_colors.items()]\n",
    "            if legend_elements:\n",
    "                ax.legend(handles=legend_elements, loc='upper right', fontsize=11, framealpha=0.9)\n",
    "\n",
    "            page_categories = [elem.metadata.get('category', 'Unknown') for elem in page_elements]\n",
    "            category_counts = Counter(page_categories)\n",
    "            stats_text = \" | \".join([f\"{cat}: {count}\" for cat, count in category_counts.most_common(3)])\n",
    "            if stats_text:\n",
    "                ax.set_title(f\"P√°gina {page_num} | {stats_text}\", fontsize=14, pad=10)\n",
    "            else:\n",
    "                ax.set_title(f\"P√°gina {page_num}\", fontsize=14, pad=10)\n",
    "\n",
    "            fig.tight_layout()\n",
    "            output_path = pdf_output_dir / f\"page-{page_num:03d}.png\"\n",
    "            fig.savefig(output_path, dpi=200, bbox_inches='tight')\n",
    "            plt.close(fig)\n",
    "            page_stats.append((page_num, len(page_elements), output_path))\n",
    "    finally:\n",
    "        doc.close()\n",
    "\n",
    "    print(f\"‚úÖ Plots por p√°gina salvos em {pdf_output_dir}\")\n",
    "    return page_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d75c559a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Persist√™ncia no Chroma desabilitada ‚Äî apenas gera√ß√£o de chunks para os plots.\n"
     ]
    }
   ],
   "source": [
    "print(\"üì¶ Persist√™ncia no Chroma desabilitada ‚Äî apenas gera√ß√£o de chunks para os plots.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6cd774e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def group_elements_by_page(elements):\n",
    "    pages_dict = defaultdict(list)\n",
    "    for elem in elements:\n",
    "        page_num = elem.metadata.get('page_number')\n",
    "        if page_num is not None:\n",
    "            pages_dict[page_num].append(elem)\n",
    "    return pages_dict\n",
    "\n",
    "def build_parent_documents(elements, pdf_path):\n",
    "    pages_dict = group_elements_by_page(elements)\n",
    "    print(f\"üìÑ Total de p√°ginas encontradas: {len(pages_dict)}\")\n",
    "    parent_documents = []\n",
    "    for page_num in sorted(pages_dict.keys()):\n",
    "        page_elements = pages_dict[page_num]\n",
    "        page_text = '\\n\\n'.join(elem.page_content for elem in page_elements)\n",
    "        first_elem = page_elements[0]\n",
    "        parent_metadata = {\n",
    "            'page_number': page_num,\n",
    "            'source': first_elem.metadata.get('source', str(pdf_path)),\n",
    "            'filename': first_elem.metadata.get('filename', pdf_path.name),\n",
    "            'source_path': str(pdf_path),\n",
    "            'total_elements': len(page_elements),\n",
    "            'type': 'parent_page'\n",
    "        }\n",
    "        parent_documents.append(Document(page_content=page_text, metadata=parent_metadata))\n",
    "    print(f\"‚úÖ {len(parent_documents)} parent documents criados\")\n",
    "    return parent_documents, pages_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "58bbd5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß SemanticChunker ativo | threshold_type= percentile  | threshold_amount= 0.2  | buffer_size= 1\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "\n",
    "hf = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "CHUNKING_STRATEGY = \"semantic\"  # \"semantic\" ou \"character\"\n",
    "\n",
    "SEMANTIC_CONFIG = {\n",
    "    'breakpoint_threshold_type': 'percentile',\n",
    "    'breakpoint_threshold_amount': 0.2,\n",
    "    'buffer_size': 1\n",
    "}\n",
    "\n",
    "CHARACTER_CONFIG = {\n",
    "    'chunk_size': 380,\n",
    "    'chunk_overlap': 60,\n",
    "    'separators': [\"\\n\\n\", \"\\n\", \" \" ]\n",
    "}\n",
    "\n",
    "def build_text_splitter():\n",
    "    if CHUNKING_STRATEGY == \"semantic\":\n",
    "        splitter = SemanticChunker(\n",
    "            embeddings=hf,\n",
    "            breakpoint_threshold_type=SEMANTIC_CONFIG['breakpoint_threshold_type'],\n",
    "            breakpoint_threshold_amount=SEMANTIC_CONFIG['breakpoint_threshold_amount'],\n",
    "            buffer_size=SEMANTIC_CONFIG['buffer_size']\n",
    ")\n",
    "        print(\n",
    "            \"üîß SemanticChunker ativo | threshold_type=\",\n",
    "            SEMANTIC_CONFIG['breakpoint_threshold_type'],\n",
    "            \" | threshold_amount=\",\n",
    "            SEMANTIC_CONFIG['breakpoint_threshold_amount'],\n",
    "            \" | buffer_size=\",\n",
    "            SEMANTIC_CONFIG['buffer_size']\n",
    ")\n",
    "        return splitter\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=CHARACTER_CONFIG['chunk_size'],\n",
    "        chunk_overlap=CHARACTER_CONFIG['chunk_overlap'],\n",
    "        separators=CHARACTER_CONFIG['separators'],\n",
    "        length_function=len\n",
    ")\n",
    "    print(\n",
    "        \"üîß RecursiveCharacterTextSplitter ativo | chunk_size=\",\n",
    "        CHARACTER_CONFIG['chunk_size'],\n",
    "        \" | chunk_overlap=\",\n",
    "        CHARACTER_CONFIG['chunk_overlap']\n",
    "    )\n",
    "    return splitter\n",
    "\n",
    "text_splitter = build_text_splitter()\n",
    "\n",
    "def create_element_map(page_elements):\n",
    "    element_map = []\n",
    "    current_pos = 0\n",
    "    for elem in page_elements:\n",
    "        text = elem.page_content or \"\"\n",
    "        start_pos = current_pos\n",
    "        end_pos = current_pos + len(text)\n",
    "        element_map.append({\n",
    "            'element': elem,\n",
    "            'start_pos': start_pos,\n",
    "            'end_pos': end_pos,\n",
    "            'text': text\n",
    "        })\n",
    "        current_pos = end_pos + 2\n",
    "    return element_map\n",
    "\n",
    "def calculate_overlap(chunk_start, chunk_end, elem_start, elem_end):\n",
    "    overlap_start = max(chunk_start, elem_start)\n",
    "    overlap_end = min(chunk_end, elem_end)\n",
    "    if overlap_start >= overlap_end:\n",
    "        return 0.0\n",
    "    elem_length = elem_end - elem_start\n",
    "    if elem_length == 0:\n",
    "        return 0.0\n",
    "    overlap_length = overlap_end - overlap_start\n",
    "    return overlap_length / elem_length\n",
    "\n",
    "def create_semantic_chunks(page_elements):\n",
    "    full_text = '\\n\\n'.join(elem.page_content or \"\" for elem in page_elements)\n",
    "    element_map = create_element_map(page_elements)\n",
    "    semantic_texts = text_splitter.split_text(full_text)\n",
    "    semantic_chunks = []\n",
    "    current_chunk_pos = 0\n",
    "    for chunk_text in semantic_texts:\n",
    "        chunk_start = full_text.find(chunk_text, current_chunk_pos)\n",
    "        if chunk_start == -1:\n",
    "            chunk_start = current_chunk_pos\n",
    "        chunk_end = chunk_start + len(chunk_text)\n",
    "        current_chunk_pos = chunk_end\n",
    "        contributing_elements = []\n",
    "        for elem_info in element_map:\n",
    "            overlap_pct = calculate_overlap(\n",
    "                chunk_start, chunk_end,\n",
    "                elem_info['start_pos'], elem_info['end_pos']\n",
    "            )\n",
    "            if overlap_pct >= 0.10:\n",
    "                contributing_elements.append({\n",
    "                    'element': elem_info['element'],\n",
    "                    'coordinates': elem_info['element'].metadata.get('coordinates'),\n",
    "                    'category': elem_info['element'].metadata.get('category'),\n",
    "                    'content': elem_info['element'].page_content,\n",
    "                    'overlap_percentage': overlap_pct\n",
    "                })\n",
    "        if not contributing_elements:\n",
    "            best_elem = None\n",
    "            best_overlap = 0\n",
    "            for elem_info in element_map:\n",
    "                overlap_pct = calculate_overlap(\n",
    "                    chunk_start, chunk_end,\n",
    "                    elem_info['start_pos'], elem_info['end_pos']\n",
    "                )\n",
    "                if overlap_pct > best_overlap:\n",
    "                    best_overlap = overlap_pct\n",
    "                    best_elem = elem_info['element']\n",
    "            if best_elem is not None:\n",
    "                contributing_elements = [{\n",
    "                    'element': best_elem,\n",
    "                    'coordinates': best_elem.metadata.get('coordinates'),\n",
    "                    'category': best_elem.metadata.get('category'),\n",
    "                    'content': best_elem.page_content,\n",
    "                    'overlap_percentage': best_overlap\n",
    "                }]\n",
    "        categories = [elem['category'] for elem in contributing_elements if elem['category']]\n",
    "        predominant_category = max(set(categories), key=categories.count) if categories else 'Unknown'\n",
    "        semantic_chunks.append({\n",
    "            'text': chunk_text,\n",
    "            'contributing_elements': contributing_elements,\n",
    "            'category': predominant_category,\n",
    "            'source_elements_count': len(contributing_elements),\n",
    "            'chunk_position': (chunk_start, chunk_end)\n",
    "        })\n",
    "    return semantic_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "50177e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "def highlight_chunks(pdf_path, elements_with_coords, output_dir, render_scale=2):\n",
    "    if not elements_with_coords:\n",
    "        print(f\"‚ö†Ô∏è Nenhum chunk sem√¢ntico para renderizar em {pdf_path.name}\")\n",
    "        return None\n",
    "    pdf_output_dir = (output_dir / pdf_path.stem)\n",
    "    pdf_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    doc = fitz.open(pdf_path)\n",
    "\n",
    "    try:\n",
    "        chunks_by_page = defaultdict(list)\n",
    "        for child_id, chunk_info in elements_with_coords.items():\n",
    "            page_num = chunk_info['page_number']\n",
    "            chunks_by_page[page_num].append(chunk_info)\n",
    "\n",
    "        print(f\"Total de p√°ginas com chunks: {len(chunks_by_page)}\")\n",
    "        for page_num in sorted(chunks_by_page.keys()):\n",
    "            page_chunks = chunks_by_page[page_num]\n",
    "            page = doc[page_num - 1]\n",
    "            pix = page.get_pixmap(matrix=fitz.Matrix(render_scale, render_scale))\n",
    "            img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(14, 18))\n",
    "            ax.imshow(img)\n",
    "            ax.axis('off')\n",
    "\n",
    "            chunk_colors = plt.cm.get_cmap('Set3')(range(len(page_chunks)))\n",
    "            for chunk_idx, chunk_info in enumerate(page_chunks):\n",
    "                color = mcolors.rgb2hex(chunk_colors[chunk_idx][:3])\n",
    "                for elem_data in chunk_info['contributing_elements']:\n",
    "                    coords = elem_data['coordinates']\n",
    "                    if coords and coords.get('points'):\n",
    "                        try:\n",
    "                            points = coords['points']\n",
    "                            x1, y1 = points[0]\n",
    "                            x2, y2 = points[2]\n",
    "                            x = x1 * render_scale\n",
    "                            y = y1 * render_scale\n",
    "                            width = (x2 - x1) * render_scale\n",
    "                            height = (y2 - y1) * render_scale\n",
    "                            rect = patches.Rectangle(\n",
    "                                (x, y), width, height,\n",
    "                                linewidth=1,\n",
    "                                edgecolor=color,\n",
    "                                facecolor=color,\n",
    "                                alpha=0.3\n",
    ")\n",
    "                            ax.add_patch(rect)\n",
    "                        except Exception:\n",
    "                            continue\n",
    "\n",
    "            ax.set_title(f\"P√°gina {page_num} - {len(page_chunks)} chunks sem√¢nticos\", fontsize=14, pad=10)\n",
    "            fig.tight_layout()\n",
    "            output_path = pdf_output_dir / f\"page-{page_num:03d}.png\"\n",
    "            fig.savefig(output_path, dpi=200, bbox_inches='tight')\n",
    "            plt.close(fig)\n",
    "\n",
    "        return pdf_output_dir\n",
    "    finally:\n",
    "        doc.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cbc8647d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìÑ Processando arquivo: 6608-06e0e21ca08fc4373941c452c916f536.pdf\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Cannot set gray non-stroke color because /'Pattern1' is an invalid float value\n",
      "WARNING: Cannot set gray non-stroke color because /'Pattern2' is an invalid float value\n",
      "WARNING: Cannot set gray non-stroke color because /'Pattern3' is an invalid float value\n",
      "WARNING: Cannot set gray non-stroke color because /'Pattern4' is an invalid float value\n",
      "WARNING: Cannot set gray non-stroke color because /'Pattern5' is an invalid float value\n",
      "WARNING: Cannot set gray non-stroke color because /'Pattern6' is an invalid float value\n",
      "WARNING: Cannot set gray non-stroke color because /'Pattern7' is an invalid float value\n",
      "WARNING: Cannot set gray non-stroke color because /'Pattern2' is an invalid float value\n",
      "WARNING: Cannot set gray non-stroke color because /'Pattern3' is an invalid float value\n",
      "WARNING: Cannot set gray non-stroke color because /'Pattern4' is an invalid float value\n",
      "WARNING: Cannot set gray non-stroke color because /'Pattern5' is an invalid float value\n",
      "WARNING: Cannot set gray non-stroke color because /'Pattern6' is an invalid float value\n",
      "WARNING: Cannot set gray non-stroke color because /'Pattern7' is an invalid float value\n",
      "WARNING: Cannot set gray non-stroke color because /'Pattern8' is an invalid float value\n",
      "WARNING: Cannot set gray non-stroke color because /'Pattern1' is an invalid float value\n",
      "WARNING: Cannot set gray non-stroke color because /'Pattern8' is an invalid float value\n",
      "WARNING: Cannot set gray non-stroke color because /'Pattern1' is an invalid float value\n",
      "WARNING: Cannot set gray non-stroke color because /'Pattern1' is an invalid float value\n",
      "WARNING: Cannot set gray non-stroke color because /'Pattern1' is an invalid float value\n",
      "WARNING: Cannot set gray non-stroke color because /'Pattern1' is an invalid float value\n",
      "WARNING: Cannot set gray non-stroke color because /'Pattern1' is an invalid float value\n",
      "WARNING: Cannot set gray non-stroke color because /'Pattern1' is an invalid float value\n",
      "WARNING: Cannot set gray non-stroke color because /'Pattern1' is an invalid float value\n",
      "WARNING: Cannot set gray non-stroke color because /'Pattern1' is an invalid float value\n",
      "WARNING: Cannot set gray non-stroke color because /'Pattern1' is an invalid float value\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_17584\\817455802.py:13: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  cmap = plt.cm.get_cmap('tab20' if n_categories <= 20 else 'hsv')\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_17584\\817455802.py:13: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  cmap = plt.cm.get_cmap('tab20' if n_categories <= 20 else 'hsv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n",
      "Number of documents loaded: 52\n",
      "‚úÖ Plots por p√°gina salvos em C:\\Users\\User\\Workplace\\not-a-rag-chat\\src\\notebooks\\outputs\\page_plots\\6608-06e0e21ca08fc4373941c452c916f536\n",
      "üìÑ Total de p√°ginas encontradas: 6\n",
      "‚úÖ 6 parent documents criados\n",
      "üîß Criando chunks sem√¢nticos preservando elementos individuais...\n",
      "‚úÖ Plots por p√°gina salvos em C:\\Users\\User\\Workplace\\not-a-rag-chat\\src\\notebooks\\outputs\\page_plots\\6608-06e0e21ca08fc4373941c452c916f536\n",
      "üìÑ Total de p√°ginas encontradas: 6\n",
      "‚úÖ 6 parent documents criados\n",
      "üîß Criando chunks sem√¢nticos preservando elementos individuais...\n",
      "  P√°gina 1: 32 elementos ‚Üí 27 chunks sem√¢nticos\n",
      "  P√°gina 1: 32 elementos ‚Üí 27 chunks sem√¢nticos\n",
      "  P√°gina 2: 12 elementos ‚Üí 9 chunks sem√¢nticos\n",
      "  P√°gina 3: 2 elementos ‚Üí 1 chunks sem√¢nticos\n",
      "  P√°gina 4: 2 elementos ‚Üí 1 chunks sem√¢nticos\n",
      "  P√°gina 5: 2 elementos ‚Üí 1 chunks sem√¢nticos\n",
      "  P√°gina 6: 2 elementos ‚Üí 1 chunks sem√¢nticos\n",
      "\n",
      "============================================================\n",
      "‚úÖ 6 parents armazenados\n",
      "‚úÖ 40 chunks sem√¢nticos gerados\n",
      "‚úÖ 40 chunks com elementos individuais preservados\n",
      "\n",
      "M√©dia de chunks por p√°gina: 6.7\n",
      "M√©dia de elementos individuais por chunk: 1.7\n",
      "Total de elementos individuais para highlighting: 69\n",
      "üíæ Registro dos chunks salvo em C:\\Users\\User\\Workplace\\not-a-rag-chat\\src\\notebooks\\outputs\\elements\\chunks\\6608-06e0e21ca08fc4373941c452c916f536_chunks.json\n",
      "Total de p√°ginas com chunks: 6\n",
      "  P√°gina 2: 12 elementos ‚Üí 9 chunks sem√¢nticos\n",
      "  P√°gina 3: 2 elementos ‚Üí 1 chunks sem√¢nticos\n",
      "  P√°gina 4: 2 elementos ‚Üí 1 chunks sem√¢nticos\n",
      "  P√°gina 5: 2 elementos ‚Üí 1 chunks sem√¢nticos\n",
      "  P√°gina 6: 2 elementos ‚Üí 1 chunks sem√¢nticos\n",
      "\n",
      "============================================================\n",
      "‚úÖ 6 parents armazenados\n",
      "‚úÖ 40 chunks sem√¢nticos gerados\n",
      "‚úÖ 40 chunks com elementos individuais preservados\n",
      "\n",
      "M√©dia de chunks por p√°gina: 6.7\n",
      "M√©dia de elementos individuais por chunk: 1.7\n",
      "Total de elementos individuais para highlighting: 69\n",
      "üíæ Registro dos chunks salvo em C:\\Users\\User\\Workplace\\not-a-rag-chat\\src\\notebooks\\outputs\\elements\\chunks\\6608-06e0e21ca08fc4373941c452c916f536_chunks.json\n",
      "Total de p√°ginas com chunks: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_17584\\2838090921.py:32: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  chunk_colors = plt.cm.get_cmap('Set3')(range(len(page_chunks)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìÑ Processando arquivo: 6608-0a1d0058940a3f53d22f922124b0e884.pdf\n",
      "================================================================================\n",
      "Warning: No languages specified, defaulting to English.\n",
      "Number of documents loaded: 57\n",
      "Warning: No languages specified, defaulting to English.\n",
      "Number of documents loaded: 57\n",
      "‚úÖ Plots por p√°gina salvos em C:\\Users\\User\\Workplace\\not-a-rag-chat\\src\\notebooks\\outputs\\page_plots\\6608-0a1d0058940a3f53d22f922124b0e884\n",
      "üìÑ Total de p√°ginas encontradas: 7\n",
      "‚úÖ 7 parent documents criados\n",
      "üîß Criando chunks sem√¢nticos preservando elementos individuais...\n",
      "‚úÖ Plots por p√°gina salvos em C:\\Users\\User\\Workplace\\not-a-rag-chat\\src\\notebooks\\outputs\\page_plots\\6608-0a1d0058940a3f53d22f922124b0e884\n",
      "üìÑ Total de p√°ginas encontradas: 7\n",
      "‚úÖ 7 parent documents criados\n",
      "üîß Criando chunks sem√¢nticos preservando elementos individuais...\n",
      "  P√°gina 1: 5 elementos ‚Üí 1 chunks sem√¢nticos\n",
      "  P√°gina 1: 5 elementos ‚Üí 1 chunks sem√¢nticos\n",
      "  P√°gina 2: 12 elementos ‚Üí 16 chunks sem√¢nticos\n",
      "  P√°gina 2: 12 elementos ‚Üí 16 chunks sem√¢nticos\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     23\u001b[39m page_stats = highlight_elements(pdf_path, elements, PAGE_PLOTS_DIR)\n\u001b[32m     24\u001b[39m parent_documents, pages_dict = build_parent_documents(elements, pdf_path)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m semantic_payload = \u001b[43mbuild_semantic_children\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparent_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpages_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m chunks_output_dir = ELEMENTS_OUTPUT_DIR / \u001b[33m\"\u001b[39m\u001b[33mchunks\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     28\u001b[39m chunks_output_dir.mkdir(parents=\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 125\u001b[39m, in \u001b[36mbuild_semantic_children\u001b[39m\u001b[34m(parent_documents, pages_dict, pdf_path)\u001b[39m\n\u001b[32m    123\u001b[39m page_num = parent_doc.metadata[\u001b[33m'\u001b[39m\u001b[33mpage_number\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    124\u001b[39m page_elements = pages_dict.get(page_num, [])\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m semantic_chunks = \u001b[43mcreate_semantic_chunks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpage_elements\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  P√°gina \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(page_elements)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m elementos ‚Üí \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(semantic_chunks)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m chunks sem√¢nticos\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m chunk_data \u001b[38;5;129;01min\u001b[39;00m semantic_chunks:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 94\u001b[39m, in \u001b[36mcreate_semantic_chunks\u001b[39m\u001b[34m(page_elements)\u001b[39m\n\u001b[32m     92\u001b[39m full_text = \u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m.join(elem.page_content \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m page_elements)\n\u001b[32m     93\u001b[39m element_map = create_element_map(page_elements)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m semantic_texts = \u001b[43mtext_splitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m semantic_chunks = []\n\u001b[32m     96\u001b[39m current_chunk_pos = \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Workplace\\not-a-rag-chat\\.venv\\Lib\\site-packages\\langchain_experimental\\text_splitter.py:231\u001b[39m, in \u001b[36mSemanticChunker.split_text\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m    226\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    227\u001b[39m     \u001b[38;5;28mself\u001b[39m.breakpoint_threshold_type == \u001b[33m\"\u001b[39m\u001b[33mgradient\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    228\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(single_sentences_list) == \u001b[32m2\u001b[39m\n\u001b[32m    229\u001b[39m ):\n\u001b[32m    230\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m single_sentences_list\n\u001b[32m--> \u001b[39m\u001b[32m231\u001b[39m distances, sentences = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_calculate_sentence_distances\u001b[49m\u001b[43m(\u001b[49m\u001b[43msingle_sentences_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.number_of_chunks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    233\u001b[39m     breakpoint_distance_threshold = \u001b[38;5;28mself\u001b[39m._threshold_from_clusters(distances)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Workplace\\not-a-rag-chat\\.venv\\Lib\\site-packages\\langchain_experimental\\text_splitter.py:203\u001b[39m, in \u001b[36mSemanticChunker._calculate_sentence_distances\u001b[39m\u001b[34m(self, single_sentences_list)\u001b[39m\n\u001b[32m    199\u001b[39m _sentences = [\n\u001b[32m    200\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33msentence\u001b[39m\u001b[33m\"\u001b[39m: x, \u001b[33m\"\u001b[39m\u001b[33mindex\u001b[39m\u001b[33m\"\u001b[39m: i} \u001b[38;5;28;01mfor\u001b[39;00m i, x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(single_sentences_list)\n\u001b[32m    201\u001b[39m ]\n\u001b[32m    202\u001b[39m sentences = combine_sentences(_sentences, \u001b[38;5;28mself\u001b[39m.buffer_size)\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcombined_sentence\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msentences\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, sentence \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sentences):\n\u001b[32m    207\u001b[39m     sentence[\u001b[33m\"\u001b[39m\u001b[33mcombined_sentence_embedding\u001b[39m\u001b[33m\"\u001b[39m] = embeddings[i]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Workplace\\not-a-rag-chat\\.venv\\Lib\\site-packages\\langchain_huggingface\\embeddings\\huggingface.py:156\u001b[39m, in \u001b[36mHuggingFaceEmbeddings.embed_documents\u001b[39m\u001b[34m(self, texts)\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34membed_documents\u001b[39m(\u001b[38;5;28mself\u001b[39m, texts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]) -> \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mfloat\u001b[39m]]:\n\u001b[32m    147\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Compute doc embeddings using a HuggingFace transformer model.\u001b[39;00m\n\u001b[32m    148\u001b[39m \n\u001b[32m    149\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    154\u001b[39m \n\u001b[32m    155\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencode_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Workplace\\not-a-rag-chat\\.venv\\Lib\\site-packages\\langchain_huggingface\\embeddings\\huggingface.py:131\u001b[39m, in \u001b[36mHuggingFaceEmbeddings._embed\u001b[39m\u001b[34m(self, texts, encode_kwargs)\u001b[39m\n\u001b[32m    129\u001b[39m     sentence_transformers.SentenceTransformer.stop_multi_process_pool(pool)\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m     embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mencode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(embeddings, \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m    138\u001b[39m     msg = (\n\u001b[32m    139\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mExpected embeddings to be a Tensor or a numpy array, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    140\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mgot a list instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    141\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Workplace\\not-a-rag-chat\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Workplace\\not-a-rag-chat\\.venv\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:1094\u001b[39m, in \u001b[36mSentenceTransformer.encode\u001b[39m\u001b[34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, truncate_dim, pool, chunk_size, **kwargs)\u001b[39m\n\u001b[32m   1091\u001b[39m features.update(extra_features)\n\u001b[32m   1093\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m-> \u001b[39m\u001b[32m1094\u001b[39m     out_features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1095\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.device.type == \u001b[33m\"\u001b[39m\u001b[33mhpu\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1096\u001b[39m         out_features = copy.deepcopy(out_features)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Workplace\\not-a-rag-chat\\.venv\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:1175\u001b[39m, in \u001b[36mSentenceTransformer.forward\u001b[39m\u001b[34m(self, input, **kwargs)\u001b[39m\n\u001b[32m   1169\u001b[39m             module_kwarg_keys = \u001b[38;5;28mself\u001b[39m.module_kwargs.get(module_name, [])\n\u001b[32m   1170\u001b[39m         module_kwargs = {\n\u001b[32m   1171\u001b[39m             key: value\n\u001b[32m   1172\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs.items()\n\u001b[32m   1173\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module_kwarg_keys \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mhasattr\u001b[39m(module, \u001b[33m\"\u001b[39m\u001b[33mforward_kwargs\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module.forward_kwargs)\n\u001b[32m   1174\u001b[39m         }\n\u001b[32m-> \u001b[39m\u001b[32m1175\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Workplace\\not-a-rag-chat\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Workplace\\not-a-rag-chat\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Workplace\\not-a-rag-chat\\.venv\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:261\u001b[39m, in \u001b[36mTransformer.forward\u001b[39m\u001b[34m(self, features, **kwargs)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    239\u001b[39m \u001b[33;03mForward pass through the transformer model.\u001b[39;00m\n\u001b[32m    240\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    257\u001b[39m \u001b[33;03m        - 'all_layer_embeddings': If the model outputs hidden states, contains embeddings from all layers\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    259\u001b[39m trans_features = {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m features.items() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model_forward_params}\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mauto_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtrans_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    262\u001b[39m token_embeddings = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    263\u001b[39m features[\u001b[33m\"\u001b[39m\u001b[33mtoken_embeddings\u001b[39m\u001b[33m\"\u001b[39m] = token_embeddings\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Workplace\\not-a-rag-chat\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Workplace\\not-a-rag-chat\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Workplace\\not-a-rag-chat\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:486\u001b[39m, in \u001b[36mMPNetModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[39m\n\u001b[32m    484\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m    485\u001b[39m embedding_output = \u001b[38;5;28mself\u001b[39m.embeddings(input_ids=input_ids, position_ids=position_ids, inputs_embeds=inputs_embeds)\n\u001b[32m--> \u001b[39m\u001b[32m486\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    494\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    495\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Workplace\\not-a-rag-chat\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Workplace\\not-a-rag-chat\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Workplace\\not-a-rag-chat\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:338\u001b[39m, in \u001b[36mMPNetEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[39m\n\u001b[32m    335\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[32m    336\u001b[39m     all_hidden_states = all_hidden_states + (hidden_states,)\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    346\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    348\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Workplace\\not-a-rag-chat\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Workplace\\not-a-rag-chat\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Workplace\\not-a-rag-chat\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:297\u001b[39m, in \u001b[36mMPNetLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, position_bias, output_attentions, **kwargs)\u001b[39m\n\u001b[32m    288\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    289\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    290\u001b[39m     hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    295\u001b[39m     **kwargs,\n\u001b[32m    296\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m297\u001b[39m     self_attention_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    301\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    303\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    304\u001b[39m     attention_output = self_attention_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    305\u001b[39m     outputs = self_attention_outputs[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add self attentions if we output attention weights\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Workplace\\not-a-rag-chat\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Workplace\\not-a-rag-chat\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Workplace\\not-a-rag-chat\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:238\u001b[39m, in \u001b[36mMPNetAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, position_bias, output_attentions, **kwargs)\u001b[39m\n\u001b[32m    229\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    230\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    231\u001b[39m     hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    236\u001b[39m     **kwargs,\n\u001b[32m    237\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m238\u001b[39m     self_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    241\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m     attention_output = \u001b[38;5;28mself\u001b[39m.LayerNorm(\u001b[38;5;28mself\u001b[39m.dropout(self_outputs[\u001b[32m0\u001b[39m]) + hidden_states)\n\u001b[32m    246\u001b[39m     outputs = (attention_output,) + self_outputs[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Workplace\\not-a-rag-chat\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Workplace\\not-a-rag-chat\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Workplace\\not-a-rag-chat\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:198\u001b[39m, in \u001b[36mMPNetSelfAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, position_bias, output_attentions, **kwargs)\u001b[39m\n\u001b[32m    195\u001b[39m new_c_shape = c.size()[:-\u001b[32m2\u001b[39m] + (\u001b[38;5;28mself\u001b[39m.all_head_size,)\n\u001b[32m    196\u001b[39m c = c.view(*new_c_shape)\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m o = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    200\u001b[39m outputs = (o, attention_probs) \u001b[38;5;28;01mif\u001b[39;00m output_attentions \u001b[38;5;28;01melse\u001b[39;00m (o,)\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Workplace\\not-a-rag-chat\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Workplace\\not-a-rag-chat\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Workplace\\not-a-rag-chat\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m    131\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import traceback\n",
    "from datetime import datetime\n",
    "\n",
    "processing_results = []\n",
    "start_time = datetime.now()\n",
    "\n",
    "for pdf_path in PDF_PATHS:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üìÑ Processando arquivo: {pdf_path.name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    try:\n",
    "        loader = UnstructuredLoader(str(pdf_path), partition_kwargs={\"languages\": [\"por\"], \"strategy\": \"hi_res\"})\n",
    "        elements = loader.load()\n",
    "        print(f\"Number of documents loaded: {len(elements)}\")\n",
    "\n",
    "        for elem in elements:\n",
    "            elem.metadata.setdefault('filename', pdf_path.name)\n",
    "            elem.metadata.setdefault('source', str(pdf_path))\n",
    "            elem.metadata['source_path'] = str(pdf_path)\n",
    "\n",
    "        serialized_path = ELEMENTS_OUTPUT_DIR / f\"{pdf_path.stem}.json\"\n",
    "        dump_elements(elements, serialized_path)\n",
    "        page_stats = highlight_elements(pdf_path, elements, PAGE_PLOTS_DIR)\n",
    "        parent_documents, pages_dict = build_parent_documents(elements, pdf_path)\n",
    "        semantic_payload = build_semantic_children(parent_documents, pages_dict, pdf_path)\n",
    "\n",
    "        chunks_output_dir = ELEMENTS_OUTPUT_DIR / \"chunks\"\n",
    "        chunks_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        chunks_json_path = chunks_output_dir / f\"{pdf_path.stem}_chunks.json\"\n",
    "\n",
    "        chunk_records = []\n",
    "        for child_id, chunk_info in semantic_payload['elements_with_coords'].items():\n",
    "            chunk_records.append({\n",
    "                \"chunk_id\": child_id,\n",
    "                \"text\": chunk_info['content'],\n",
    "                \"category\": chunk_info['category'],\n",
    "                \"page_number\": chunk_info['page_number'],\n",
    "                \"source_elements_count\": chunk_info['source_elements_count'],\n",
    "                \"chunk_position\": chunk_info['chunk_position'],\n",
    "                \"contributing_elements\": [\n",
    "                    {\n",
    "                        \"coordinates\": elem_data['coordinates'],\n",
    "                        \"category\": elem_data['category'],\n",
    "                        \"content\": elem_data['content'],\n",
    "                        \"overlap_percentage\": elem_data['overlap_percentage']\n",
    "                    }\n",
    "                    for elem_data in chunk_info['contributing_elements']\n",
    "                ]\n",
    "            })\n",
    "\n",
    "        with open(chunks_json_path, \"w\", encoding=\"utf-8\") as fp:\n",
    "            json.dump(chunk_records, fp, ensure_ascii=False, indent=2)\n",
    "        print(f\"üíæ Registro dos chunks salvo em {chunks_json_path}\")\n",
    "\n",
    "        chunk_plots_dir = highlight_chunks(pdf_path, semantic_payload['elements_with_coords'], CHUNK_PLOTS_DIR)\n",
    "\n",
    "        chunk_pages = {info['page_number'] for info in semantic_payload['elements_with_coords'].values()}\n",
    "        processing_results.append({\n",
    "            'pdf': pdf_path,\n",
    "            'elements_path': serialized_path,\n",
    "            'chunks_path': chunks_json_path,\n",
    "            'page_plots': page_stats,\n",
    "            'chunk_plots_dir': chunk_plots_dir,\n",
    "            'total_elements': len(elements),\n",
    "            'parent_documents': len(parent_documents),\n",
    "            'semantic_children': len(semantic_payload['semantic_children']),\n",
    "            'chunk_pages': len(chunk_pages)\n",
    "        })\n",
    "    except Exception as exc:\n",
    "        print(f\"‚ö†Ô∏è Erro ao processar {pdf_path.name}: {exc}\")\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "elapsed = datetime.now() - start_time\n",
    "print(f\"\\n‚è±Ô∏è Pipeline conclu√≠do em {elapsed} (hh:mm:ss)\")\n",
    "\n",
    "if processing_results:\n",
    "    print(\"\\nResumo por arquivo:\")\n",
    "    for result in processing_results:\n",
    "        page_plot_count = len(result['page_plots']) if result['page_plots'] else 0\n",
    "        chunk_plot_path = str(result['chunk_plots_dir']) if result['chunk_plots_dir'] else 'N/A'\n",
    "        summary_line = \"- {name}: {elements} elementos, {parents} parents, {chunks} chunks, plots de p√°ginas: {pages}, plots de chunks: {chunk_pages}.\".format(\n",
    "            name=result['pdf'].name,\n",
    "            elements=result['total_elements'],\n",
    "            parents=result['parent_documents'],\n",
    "            chunks=result['semantic_children'],\n",
    "            pages=page_plot_count,\n",
    "            chunk_pages=result['chunk_pages']\n",
    "        )\n",
    "        print(summary_line)\n",
    "        print(f\"  JSON de elementos: {result['elements_path']}\")\n",
    "        print(f\"  JSON de chunks: {result['chunks_path']}\")\n",
    "        if page_plot_count:\n",
    "            first_page_plot = result['page_plots'][0][2]\n",
    "            print(f\"  Exemplos de plots de p√°gina: {first_page_plot.parent}\")\n",
    "        print(f\"  Plots de chunks: {chunk_plot_path}\")\n",
    "else:\n",
    "    print(\"Nenhum arquivo foi processado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff58b05",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expression expected after dictionary key and ':' (3434448406.py, line 7)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m\"litellm_params\":\u001b[39m\n                    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m expression expected after dictionary key and ':'\n"
     ]
    }
   ],
   "source": [
    "# # P√≥s processamento: \n",
    "\n",
    "# from langchain_litellm import ChatLiteLLM\n",
    "# from langchain_openai import ChatOpenAI\n",
    "\n",
    "# chat = ChatOpenAI(\n",
    "#     openai_api_base=\"http://localhost:4000\",  # Your proxy URL\n",
    "#     model=\"gpt-4o\",\n",
    "#     temperature=0.7,\n",
    "#     extra_body={\n",
    "#         \"metadata\": {\n",
    "#             \"tags\": [\"proxy\", \"team-alpha\", \"feature-flagged\"],\n",
    "#             \"generation_name\": \"customer-onboarding\",\n",
    "#             \"trace_user_id\": \"user-12345\"\n",
    "#         }\n",
    "#     }\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6907b918",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "not-a-rag-chat (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
