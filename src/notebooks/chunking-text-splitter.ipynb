{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02943bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Diret√≥rio de dados: C:\\Users\\User\\Workplace\\not-a-rag-chat\\src\\notebooks\\data\n",
      "üñºÔ∏è Diret√≥rio de plots por p√°gina: C:\\Users\\User\\Workplace\\not-a-rag-chat\\src\\notebooks\\outputs-text-splitter\\page_plots\n",
      "üß© Diret√≥rio de plots de chunks: C:\\Users\\User\\Workplace\\not-a-rag-chat\\src\\notebooks\\outputs-text-splitter\\chunk_plots\n",
      "üìÑ PDFs detectados: 9\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from langchain_unstructured import UnstructuredLoader\n",
    "\n",
    "# Descobrir diret√≥rios relevantes a partir do contexto atual\n",
    "_CWD = Path.cwd()\n",
    "_DATA_DIR_CANDIDATES = [\n",
    "    _CWD / \"data\",\n",
    "    _CWD / \"src/notebooks/data\"\n",
    "]\n",
    "\n",
    "for candidate in _DATA_DIR_CANDIDATES:\n",
    "    if candidate.exists():\n",
    "        DATA_DIR = candidate.resolve()\n",
    "        break\n",
    "else:\n",
    "    searched = \", \".join(str(c.resolve()) for c in _DATA_DIR_CANDIDATES)\n",
    "    raise FileNotFoundError(f\"N√£o foi poss√≠vel localizar o diret√≥rio de PDFs. Caminhos testados: {searched}\")\n",
    "\n",
    "OUTPUT_ROOT = DATA_DIR.parent / \"outputs-text-splitter\"\n",
    "ELEMENTS_OUTPUT_DIR = OUTPUT_ROOT / \"elements\"\n",
    "PAGE_PLOTS_DIR = OUTPUT_ROOT / \"page_plots\"\n",
    "CHUNK_PLOTS_DIR = OUTPUT_ROOT / \"chunk_plots\"\n",
    "\n",
    "for directory in (OUTPUT_ROOT, ELEMENTS_OUTPUT_DIR, PAGE_PLOTS_DIR, CHUNK_PLOTS_DIR):\n",
    "    directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PDF_PATHS = sorted({p.resolve() for pattern in (\"*.pdf\", \"*.PDF\") for p in DATA_DIR.glob(pattern)})\n",
    "\n",
    "if not PDF_PATHS:\n",
    "    raise FileNotFoundError(f\"Nenhum PDF encontrado em {DATA_DIR}\")\n",
    "\n",
    "print(f\"üìÅ Diret√≥rio de dados: {DATA_DIR}\")\n",
    "print(f\"üñºÔ∏è Diret√≥rio de plots por p√°gina: {PAGE_PLOTS_DIR}\")\n",
    "print(f\"üß© Diret√≥rio de plots de chunks: {CHUNK_PLOTS_DIR}\")\n",
    "print(f\"üìÑ PDFs detectados: {len(PDF_PATHS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af63666b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilit√°rio para serializar elementos carregados\n",
    "import json\n",
    "\n",
    "def _element_to_serializable(el):\n",
    "    # Prefer pydantic v2 model_dump, then v1 dict, then __dict__ fallback\n",
    "    if hasattr(el, \"model_dump\"):\n",
    "        data = el.model_dump()\n",
    "    elif hasattr(el, \"dict\"):\n",
    "        data = el.dict()\n",
    "    elif hasattr(el, \"__dict__\"):\n",
    "        data = el.__dict__\n",
    "    else:\n",
    "        return str(el)\n",
    "\n",
    "    def _convert(obj):\n",
    "        if isinstance(obj, dict):\n",
    "            return {k: _convert(v) for k, v in obj.items()}\n",
    "        if isinstance(obj, (list, tuple)):\n",
    "            return [_convert(v) for v in obj]\n",
    "        if isinstance(obj, (str, int, float, bool)) or obj is None:\n",
    "            return obj\n",
    "        try:\n",
    "            return [_convert(v) for v in obj]\n",
    "        except Exception:\n",
    "            return str(obj)\n",
    "\n",
    "    return _convert(data)\n",
    "\n",
    "def dump_elements(elements, destination_path):\n",
    "    destination_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(destination_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump([_element_to_serializable(element) for element in elements], f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a6e479a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from collections import Counter\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "def generate_category_colors(categories):\n",
    "    \"\"\"Gera cores distintas para cada categoria usando colormap.\"\"\"\n",
    "    if not categories:\n",
    "        return {}\n",
    "    n_categories = len(categories)\n",
    "    cmap = plt.cm.get_cmap('tab20' if n_categories <= 20 else 'hsv')\n",
    "    colors = {}\n",
    "    for idx, cat in enumerate(categories):\n",
    "        color_rgba = cmap(idx / max(n_categories - 1, 1))\n",
    "        colors[cat] = mcolors.rgb2hex(color_rgba[:3])\n",
    "    return colors\n",
    "\n",
    "def highlight_elements(pdf_path, elements, output_dir, render_scale=2):\n",
    "    pdf_output_dir = (output_dir / pdf_path.stem)\n",
    "    pdf_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    doc = fitz.open(pdf_path)\n",
    "    categories = sorted(set(elem.metadata.get('category', 'Unknown') for elem in elements))\n",
    "    category_colors = generate_category_colors(categories)\n",
    "    page_stats = []\n",
    "\n",
    "    try:\n",
    "        for page_index in range(doc.page_count):\n",
    "            page_num = page_index + 1\n",
    "            page_elements = [elem for elem in elements if elem.metadata.get('page_number') == page_num]\n",
    "            if not page_elements:\n",
    "                continue\n",
    "\n",
    "            page = doc[page_index]\n",
    "            pix = page.get_pixmap(matrix=fitz.Matrix(render_scale, render_scale))\n",
    "            img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(14, 18))\n",
    "            ax.imshow(img)\n",
    "            ax.axis('off')\n",
    "\n",
    "            for elem in page_elements:\n",
    "                coords = elem.metadata.get('coordinates')\n",
    "                category = elem.metadata.get('category', 'Unknown')\n",
    "                if coords and coords.get('points'):\n",
    "                    try:\n",
    "                        points = coords['points']\n",
    "                        x1, y1 = points[0]\n",
    "                        x2, y2 = points[2]\n",
    "                        x = x1 * render_scale\n",
    "                        y = y1 * render_scale\n",
    "                        width = (x2 - x1) * render_scale\n",
    "                        height = (y2 - y1) * render_scale\n",
    "                        color = category_colors.get(category, '#CCCCCC')\n",
    "                        rect = patches.Rectangle(\n",
    "                    (x, y), width, height,\n",
    "                    linewidth=0,\n",
    "                    edgecolor='none',\n",
    "                    facecolor=color,\n",
    "                    alpha=0.4\n",
    "                )\n",
    "                        ax.add_patch(rect)\n",
    "                    except Exception:\n",
    "                        continue\n",
    "\n",
    "            legend_elements = [patches.Patch(facecolor=color, label=cat, alpha=0.6) for cat, color in category_colors.items()]\n",
    "            if legend_elements:\n",
    "                ax.legend(handles=legend_elements, loc='upper right', fontsize=11, framealpha=0.9)\n",
    "\n",
    "            page_categories = [elem.metadata.get('category', 'Unknown') for elem in page_elements]\n",
    "            category_counts = Counter(page_categories)\n",
    "            stats_text = \" | \".join([f\"{cat}: {count}\" for cat, count in category_counts.most_common(3)])\n",
    "            if stats_text:\n",
    "                ax.set_title(f\"P√°gina {page_num} | {stats_text}\", fontsize=14, pad=10)\n",
    "            else:\n",
    "                ax.set_title(f\"P√°gina {page_num}\", fontsize=14, pad=10)\n",
    "\n",
    "            fig.tight_layout()\n",
    "            output_path = pdf_output_dir / f\"page-{page_num:03d}.png\"\n",
    "            fig.savefig(output_path, dpi=200, bbox_inches='tight')\n",
    "            plt.close(fig)\n",
    "            page_stats.append((page_num, len(page_elements), output_path))\n",
    "    finally:\n",
    "        doc.close()\n",
    "\n",
    "    print(f\"‚úÖ Plots por p√°gina salvos em {pdf_output_dir}\")\n",
    "    return page_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d75c559a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Persist√™ncia no Chroma desabilitada ‚Äî apenas gera√ß√£o de chunks para os plots.\n"
     ]
    }
   ],
   "source": [
    "print(\"üì¶ Persist√™ncia no Chroma desabilitada ‚Äî apenas gera√ß√£o de chunks para os plots.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6cd774e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def group_elements_by_page(elements):\n",
    "    pages_dict = defaultdict(list)\n",
    "    for elem in elements:\n",
    "        page_num = elem.metadata.get('page_number')\n",
    "        if page_num is not None:\n",
    "            pages_dict[page_num].append(elem)\n",
    "    return pages_dict\n",
    "\n",
    "def build_parent_documents(elements, pdf_path):\n",
    "    pages_dict = group_elements_by_page(elements)\n",
    "    print(f\"üìÑ Total de p√°ginas encontradas: {len(pages_dict)}\")\n",
    "    parent_documents = []\n",
    "    for page_num in sorted(pages_dict.keys()):\n",
    "        page_elements = pages_dict[page_num]\n",
    "        page_text = '\\n\\n'.join(elem.page_content for elem in page_elements)\n",
    "        first_elem = page_elements[0]\n",
    "        parent_metadata = {\n",
    "            'page_number': page_num,\n",
    "            'source': first_elem.metadata.get('source', str(pdf_path)),\n",
    "            'filename': first_elem.metadata.get('filename', pdf_path.name),\n",
    "            'source_path': str(pdf_path),\n",
    "            'total_elements': len(page_elements),\n",
    "            'type': 'parent_page'\n",
    "        }\n",
    "        parent_documents.append(Document(page_content=page_text, metadata=parent_metadata))\n",
    "    print(f\"‚úÖ {len(parent_documents)} parent documents criados\")\n",
    "    return parent_documents, pages_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "58bbd5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=320,\n",
    "    chunk_overlap=0,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\"],\n",
    "    is_separator_regex=True\n",
    ")\n",
    "\n",
    "def create_element_map(page_elements):\n",
    "    element_map = []\n",
    "    current_pos = 0\n",
    "    for elem in page_elements:\n",
    "        text = elem.page_content or \"\"\n",
    "        start_pos = current_pos\n",
    "        end_pos = current_pos + len(text)\n",
    "        element_map.append({\n",
    "            'element': elem,\n",
    "            'start_pos': start_pos,\n",
    "            'end_pos': end_pos,\n",
    "            'text': text\n",
    "        })\n",
    "        current_pos = end_pos + 2\n",
    "    return element_map\n",
    "\n",
    "def calculate_overlap(chunk_start, chunk_end, elem_start, elem_end):\n",
    "    overlap_start = max(chunk_start, elem_start)\n",
    "    overlap_end = min(chunk_end, elem_end)\n",
    "    if overlap_start >= overlap_end:\n",
    "        return 0.0\n",
    "    elem_length = elem_end - elem_start\n",
    "    if elem_length == 0:\n",
    "        return 0.0\n",
    "    overlap_length = overlap_end - overlap_start\n",
    "    return overlap_length / elem_length\n",
    "\n",
    "def create_semantic_chunks(page_elements):\n",
    "    full_text = '\\n\\n'.join(elem.page_content or \"\" for elem in page_elements)\n",
    "    element_map = create_element_map(page_elements)\n",
    "    semantic_texts = text_splitter.split_text(full_text)\n",
    "    semantic_chunks = []\n",
    "    current_chunk_pos = 0\n",
    "    for chunk_text in semantic_texts:\n",
    "        chunk_start = full_text.find(chunk_text, current_chunk_pos)\n",
    "        if chunk_start == -1:\n",
    "            chunk_start = current_chunk_pos\n",
    "        chunk_end = chunk_start + len(chunk_text)\n",
    "        current_chunk_pos = chunk_end\n",
    "        contributing_elements = []\n",
    "        for elem_info in element_map:\n",
    "            overlap_pct = calculate_overlap(\n",
    "                chunk_start, chunk_end,\n",
    "                elem_info['start_pos'], elem_info['end_pos']\n",
    "            )\n",
    "            if overlap_pct >= 0.10:\n",
    "                contributing_elements.append({\n",
    "                    'element': elem_info['element'],\n",
    "                    'coordinates': elem_info['element'].metadata.get('coordinates'),\n",
    "                    'category': elem_info['element'].metadata.get('category'),\n",
    "                    'content': elem_info['element'].page_content,\n",
    "                    'overlap_percentage': overlap_pct\n",
    "                })\n",
    "        if not contributing_elements:\n",
    "            best_elem = None\n",
    "            best_overlap = 0\n",
    "            for elem_info in element_map:\n",
    "                overlap_pct = calculate_overlap(\n",
    "                    chunk_start, chunk_end,\n",
    "                    elem_info['start_pos'], elem_info['end_pos']\n",
    "                )\n",
    "                if overlap_pct > best_overlap:\n",
    "                    best_overlap = overlap_pct\n",
    "                    best_elem = elem_info['element']\n",
    "            if best_elem is not None:\n",
    "                contributing_elements = [{\n",
    "                    'element': best_elem,\n",
    "                    'coordinates': best_elem.metadata.get('coordinates'),\n",
    "                    'category': best_elem.metadata.get('category'),\n",
    "                    'content': best_elem.page_content,\n",
    "                    'overlap_percentage': best_overlap\n",
    "                }]\n",
    "        categories = [elem['category'] for elem in contributing_elements if elem['category']]\n",
    "        predominant_category = max(set(categories), key=categories.count) if categories else 'Unknown'\n",
    "        semantic_chunks.append({\n",
    "            'text': chunk_text,\n",
    "            'contributing_elements': contributing_elements,\n",
    "            'category': predominant_category,\n",
    "            'source_elements_count': len(contributing_elements),\n",
    "            'chunk_position': (chunk_start, chunk_end)\n",
    "        })\n",
    "    return semantic_chunks\n",
    "\n",
    "def build_semantic_children(parent_documents, pages_dict, pdf_path):\n",
    "    parent_store = {}\n",
    "    child_to_parent_map = {}\n",
    "    elements_with_coords = {}\n",
    "    semantic_children = []\n",
    "\n",
    "    print(\"üîß Criando chunks sem√¢nticos preservando elementos individuais...\")\n",
    "    for parent_doc in parent_documents:\n",
    "        parent_id = str(uuid.uuid4())\n",
    "        parent_doc.metadata['doc_id'] = parent_id\n",
    "        parent_store[parent_id] = parent_doc\n",
    "        page_num = parent_doc.metadata['page_number']\n",
    "        page_elements = pages_dict.get(page_num, [])\n",
    "        semantic_chunks = create_semantic_chunks(page_elements)\n",
    "        print(f\"  P√°gina {page_num}: {len(page_elements)} elementos ‚Üí {len(semantic_chunks)} chunks sem√¢nticos\")\n",
    "        for chunk_data in semantic_chunks:\n",
    "            child_doc = Document(\n",
    "                page_content=chunk_data['text'],\n",
    "                metadata={\n",
    "                    'page_number': page_num,\n",
    "                    'category': chunk_data['category'],\n",
    "                    'source': parent_doc.metadata.get('source', str(pdf_path)),\n",
    "                    'filename': parent_doc.metadata.get('filename', pdf_path.name),\n",
    "                    'source_path': parent_doc.metadata.get('source_path', str(pdf_path))\n",
    "                }\n",
    "            )\n",
    "            child_id = str(uuid.uuid4())\n",
    "            child_doc.metadata['doc_id'] = parent_id\n",
    "            child_doc.metadata['type'] = 'child_semantic'\n",
    "            child_doc.metadata['child_id'] = child_id\n",
    "\n",
    "            contributing_elements_json = []\n",
    "            for elem_data in chunk_data['contributing_elements']:\n",
    "                elem_json = {\n",
    "                    'coordinates': elem_data['coordinates'],\n",
    "                    'category': elem_data['category'],\n",
    "                    'content': (elem_data['content'] or \"\")[:100],\n",
    "                    'overlap_percentage': elem_data['overlap_percentage']\n",
    "                }\n",
    "                contributing_elements_json.append(elem_json)\n",
    "            child_doc.metadata['contributing_elements_json'] = json.dumps(contributing_elements_json)\n",
    "\n",
    "            child_to_parent_map[child_id] = parent_id\n",
    "            semantic_children.append(child_doc)\n",
    "            elements_with_coords[child_id] = {\n",
    "                'content': chunk_data['text'],\n",
    "                'contributing_elements': chunk_data['contributing_elements'],\n",
    "                'category': chunk_data['category'],\n",
    "                'page_number': page_num,\n",
    "                'source_elements_count': chunk_data['source_elements_count'],\n",
    "                'chunk_position': chunk_data['chunk_position']\n",
    "            }\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"‚úÖ {len(parent_documents)} parents armazenados\")\n",
    "    print(f\"‚úÖ {len(semantic_children)} chunks sem√¢nticos gerados\")\n",
    "    print(f\"‚úÖ {len(elements_with_coords)} chunks com elementos individuais preservados\")\n",
    "    if parent_documents:\n",
    "        print(f\"\\nM√©dia de chunks por p√°gina: {len(semantic_children) / len(parent_documents):.1f}\")\n",
    "    total_individual_elements = sum(len(ec['contributing_elements']) for ec in elements_with_coords.values())\n",
    "    avg_elements_per_chunk = total_individual_elements / len(elements_with_coords) if elements_with_coords else 0\n",
    "    print(f\"M√©dia de elementos individuais por chunk: {avg_elements_per_chunk:.1f}\")\n",
    "    print(f\"Total de elementos individuais para highlighting: {total_individual_elements}\")\n",
    "\n",
    "    return {\n",
    "        'parent_store': parent_store,\n",
    "        'child_to_parent_map': child_to_parent_map,\n",
    "        'elements_with_coords': elements_with_coords,\n",
    "        'semantic_children': semantic_children\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "50177e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "def highlight_chunks(pdf_path, elements_with_coords, output_dir, render_scale=2):\n",
    "    if not elements_with_coords:\n",
    "        print(f\"‚ö†Ô∏è Nenhum chunk sem√¢ntico para renderizar em {pdf_path.name}\")\n",
    "        return None\n",
    "    pdf_output_dir = (output_dir / pdf_path.stem)\n",
    "    pdf_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    doc = fitz.open(pdf_path)\n",
    "\n",
    "    try:\n",
    "        chunks_by_page = defaultdict(list)\n",
    "        for child_id, chunk_info in elements_with_coords.items():\n",
    "            page_num = chunk_info['page_number']\n",
    "            chunks_by_page[page_num].append(chunk_info)\n",
    "\n",
    "        print(f\"Total de p√°ginas com chunks: {len(chunks_by_page)}\")\n",
    "        for page_num in sorted(chunks_by_page.keys()):\n",
    "            page_chunks = chunks_by_page[page_num]\n",
    "            page = doc[page_num - 1]\n",
    "            pix = page.get_pixmap(matrix=fitz.Matrix(render_scale, render_scale))\n",
    "            img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(14, 18))\n",
    "            ax.imshow(img)\n",
    "            ax.axis('off')\n",
    "\n",
    "            chunk_colors = plt.cm.get_cmap('Set3')(range(len(page_chunks)))\n",
    "            for chunk_idx, chunk_info in enumerate(page_chunks):\n",
    "                color = mcolors.rgb2hex(chunk_colors[chunk_idx][:3])\n",
    "                for elem_data in chunk_info['contributing_elements']:\n",
    "                    coords = elem_data['coordinates']\n",
    "                    if coords and coords.get('points'):\n",
    "                        try:\n",
    "                            points = coords['points']\n",
    "                            x1, y1 = points[0]\n",
    "                            x2, y2 = points[2]\n",
    "                            x = x1 * render_scale\n",
    "                            y = y1 * render_scale\n",
    "                            width = (x2 - x1) * render_scale\n",
    "                            height = (y2 - y1) * render_scale\n",
    "                            rect = patches.Rectangle(\n",
    "                                (x, y), width, height,\n",
    "                                linewidth=1,\n",
    "                                edgecolor=color,\n",
    "                                facecolor=color,\n",
    "                                alpha=0.3\n",
    ")\n",
    "                            ax.add_patch(rect)\n",
    "                        except Exception:\n",
    "                            continue\n",
    "\n",
    "            ax.set_title(f\"P√°gina {page_num} - {len(page_chunks)} chunks sem√¢nticos\", fontsize=14, pad=10)\n",
    "            fig.tight_layout()\n",
    "            output_path = pdf_output_dir / f\"page-{page_num:03d}.png\"\n",
    "            fig.savefig(output_path, dpi=200, bbox_inches='tight')\n",
    "            plt.close(fig)\n",
    "\n",
    "        return pdf_output_dir\n",
    "    finally:\n",
    "        doc.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc8647d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìÑ Processando arquivo: 6608-06e0e21ca08fc4373941c452c916f536.pdf\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Cannot set gray non-stroke color because /'Pattern1' is an invalid float value\n",
      "WARNING: Cannot set gray non-stroke color because /'Pattern2' is an invalid float value\n",
      "WARNING: Cannot set gray non-stroke color because /'Pattern3' is an invalid float value\n",
      "WARNING: Cannot set gray non-stroke color because /'Pattern4' is an invalid float value\n",
      "WARNING: Cannot set gray non-stroke color because /'Pattern5' is an invalid float value\n",
      "WARNING: Cannot set gray non-stroke color because /'Pattern6' is an invalid float value\n",
      "WARNING: Cannot set gray non-stroke color because /'Pattern7' is an invalid float value\n",
      "WARNING: Cannot set gray non-stroke color because /'Pattern8' is an invalid float value\n",
      "WARNING: Cannot set gray non-stroke color because /'Pattern1' is an invalid float value\n",
      "WARNING: Cannot set gray non-stroke color because /'Pattern1' is an invalid float value\n",
      "WARNING: Cannot set gray non-stroke color because /'Pattern1' is an invalid float value\n",
      "WARNING: Cannot set gray non-stroke color because /'Pattern1' is an invalid float value\n",
      "WARNING: Cannot set gray non-stroke color because /'Pattern1' is an invalid float value\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_20584\\817455802.py:13: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  cmap = plt.cm.get_cmap('tab20' if n_categories <= 20 else 'hsv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n",
      "Number of documents loaded: 52\n",
      "‚úÖ Plots por p√°gina salvos em C:\\Users\\User\\Workplace\\not-a-rag-chat\\src\\notebooks\\outputs-text-splitter\\page_plots\\6608-06e0e21ca08fc4373941c452c916f536\n",
      "üìÑ Total de p√°ginas encontradas: 6\n",
      "‚úÖ 6 parent documents criados\n",
      "üîß Criando chunks sem√¢nticos preservando elementos individuais...\n",
      "  P√°gina 1: 32 elementos ‚Üí 8 chunks sem√¢nticos\n",
      "  P√°gina 2: 12 elementos ‚Üí 2 chunks sem√¢nticos\n",
      "  P√°gina 3: 2 elementos ‚Üí 1 chunks sem√¢nticos\n",
      "  P√°gina 4: 2 elementos ‚Üí 1 chunks sem√¢nticos\n",
      "  P√°gina 5: 2 elementos ‚Üí 1 chunks sem√¢nticos\n",
      "  P√°gina 6: 2 elementos ‚Üí 1 chunks sem√¢nticos\n",
      "\n",
      "============================================================\n",
      "‚úÖ 6 parents armazenados\n",
      "‚úÖ 14 chunks sem√¢nticos gerados\n",
      "‚úÖ 14 chunks com elementos individuais preservados\n",
      "\n",
      "M√©dia de chunks por p√°gina: 2.3\n",
      "M√©dia de elementos individuais por chunk: 3.7\n",
      "Total de elementos individuais para highlighting: 52\n",
      "Total de p√°ginas com chunks: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_20584\\2838090921.py:32: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  chunk_colors = plt.cm.get_cmap('Set3')(range(len(page_chunks)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìÑ Processando arquivo: 6608-0a1d0058940a3f53d22f922124b0e884.pdf\n",
      "================================================================================\n",
      "Warning: No languages specified, defaulting to English.\n",
      "Number of documents loaded: 57\n",
      "‚úÖ Plots por p√°gina salvos em C:\\Users\\User\\Workplace\\not-a-rag-chat\\src\\notebooks\\outputs-text-splitter\\page_plots\\6608-0a1d0058940a3f53d22f922124b0e884\n",
      "üìÑ Total de p√°ginas encontradas: 7\n",
      "‚úÖ 7 parent documents criados\n",
      "üîß Criando chunks sem√¢nticos preservando elementos individuais...\n",
      "  P√°gina 1: 5 elementos ‚Üí 3 chunks sem√¢nticos\n",
      "  P√°gina 2: 12 elementos ‚Üí 8 chunks sem√¢nticos\n",
      "  P√°gina 3: 5 elementos ‚Üí 4 chunks sem√¢nticos\n",
      "  P√°gina 4: 12 elementos ‚Üí 8 chunks sem√¢nticos\n",
      "  P√°gina 5: 5 elementos ‚Üí 5 chunks sem√¢nticos\n",
      "  P√°gina 6: 13 elementos ‚Üí 9 chunks sem√¢nticos\n",
      "  P√°gina 7: 5 elementos ‚Üí 5 chunks sem√¢nticos\n",
      "\n",
      "============================================================\n",
      "‚úÖ 7 parents armazenados\n",
      "‚úÖ 42 chunks sem√¢nticos gerados\n",
      "‚úÖ 42 chunks com elementos individuais preservados\n",
      "\n",
      "M√©dia de chunks por p√°gina: 6.0\n",
      "M√©dia de elementos individuais por chunk: 1.4\n",
      "Total de elementos individuais para highlighting: 57\n",
      "Total de p√°ginas com chunks: 7\n",
      "\n",
      "================================================================================\n",
      "üìÑ Processando arquivo: 6608-0a2e8a9ec5446b75ee84f3b02c5ef686.pdf\n",
      "================================================================================\n",
      "Warning: No languages specified, defaulting to English.\n",
      "Number of documents loaded: 41\n",
      "‚úÖ Plots por p√°gina salvos em C:\\Users\\User\\Workplace\\not-a-rag-chat\\src\\notebooks\\outputs-text-splitter\\page_plots\\6608-0a2e8a9ec5446b75ee84f3b02c5ef686\n",
      "üìÑ Total de p√°ginas encontradas: 1\n",
      "‚úÖ 1 parent documents criados\n",
      "üîß Criando chunks sem√¢nticos preservando elementos individuais...\n",
      "  P√°gina 1: 41 elementos ‚Üí 4 chunks sem√¢nticos\n",
      "\n",
      "============================================================\n",
      "‚úÖ 1 parents armazenados\n",
      "‚úÖ 4 chunks sem√¢nticos gerados\n",
      "‚úÖ 4 chunks com elementos individuais preservados\n",
      "\n",
      "M√©dia de chunks por p√°gina: 4.0\n",
      "M√©dia de elementos individuais por chunk: 10.2\n",
      "Total de elementos individuais para highlighting: 41\n",
      "Total de p√°ginas com chunks: 1\n",
      "\n",
      "================================================================================\n",
      "üìÑ Processando arquivo: 6608-0a5f52f754cb4976bc8685974ac6a916.pdf\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: The PDF <_io.BufferedReader name='C:\\\\Users\\\\User\\\\Workplace\\\\not-a-rag-chat\\\\src\\\\notebooks\\\\data\\\\6608-0a5f52f754cb4976bc8685974ac6a916.pdf'> contains a metadata field indicating that it should not allow text extraction. Ignoring this field and proceeding. Use the check_extractable if you want to raise an error in this case\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n",
      "Number of documents loaded: 19\n",
      "‚úÖ Plots por p√°gina salvos em C:\\Users\\User\\Workplace\\not-a-rag-chat\\src\\notebooks\\outputs-text-splitter\\page_plots\\6608-0a5f52f754cb4976bc8685974ac6a916\n",
      "üìÑ Total de p√°ginas encontradas: 1\n",
      "‚úÖ 1 parent documents criados\n",
      "üîß Criando chunks sem√¢nticos preservando elementos individuais...\n",
      "  P√°gina 1: 19 elementos ‚Üí 6 chunks sem√¢nticos\n",
      "\n",
      "============================================================\n",
      "‚úÖ 1 parents armazenados\n",
      "‚úÖ 6 chunks sem√¢nticos gerados\n",
      "‚úÖ 6 chunks com elementos individuais preservados\n",
      "\n",
      "M√©dia de chunks por p√°gina: 6.0\n",
      "M√©dia de elementos individuais por chunk: 3.2\n",
      "Total de elementos individuais para highlighting: 19\n",
      "Total de p√°ginas com chunks: 1\n",
      "\n",
      "================================================================================\n",
      "üìÑ Processando arquivo: 6608-0a8352b90299a624fec5b492f07d38b1.pdf\n",
      "================================================================================\n",
      "Warning: No languages specified, defaulting to English.\n",
      "Number of documents loaded: 30\n",
      "‚úÖ Plots por p√°gina salvos em C:\\Users\\User\\Workplace\\not-a-rag-chat\\src\\notebooks\\outputs-text-splitter\\page_plots\\6608-0a8352b90299a624fec5b492f07d38b1\n",
      "üìÑ Total de p√°ginas encontradas: 1\n",
      "‚úÖ 1 parent documents criados\n",
      "üîß Criando chunks sem√¢nticos preservando elementos individuais...\n",
      "  P√°gina 1: 30 elementos ‚Üí 6 chunks sem√¢nticos\n",
      "\n",
      "============================================================\n",
      "‚úÖ 1 parents armazenados\n",
      "‚úÖ 6 chunks sem√¢nticos gerados\n",
      "‚úÖ 6 chunks com elementos individuais preservados\n",
      "\n",
      "M√©dia de chunks por p√°gina: 6.0\n",
      "M√©dia de elementos individuais por chunk: 5.0\n",
      "Total de elementos individuais para highlighting: 30\n",
      "Total de p√°ginas com chunks: 1\n",
      "\n",
      "================================================================================\n",
      "üìÑ Processando arquivo: 6608-0af9a89a5bd79cb8becd09af9eaeeac0.PDF\n",
      "================================================================================\n",
      "Warning: No languages specified, defaulting to English.\n",
      "Number of documents loaded: 25\n",
      "‚úÖ Plots por p√°gina salvos em C:\\Users\\User\\Workplace\\not-a-rag-chat\\src\\notebooks\\outputs-text-splitter\\page_plots\\6608-0af9a89a5bd79cb8becd09af9eaeeac0\n",
      "üìÑ Total de p√°ginas encontradas: 2\n",
      "‚úÖ 2 parent documents criados\n",
      "üîß Criando chunks sem√¢nticos preservando elementos individuais...\n",
      "  P√°gina 1: 16 elementos ‚Üí 8 chunks sem√¢nticos\n",
      "  P√°gina 2: 9 elementos ‚Üí 9 chunks sem√¢nticos\n",
      "\n",
      "============================================================\n",
      "‚úÖ 2 parents armazenados\n",
      "‚úÖ 17 chunks sem√¢nticos gerados\n",
      "‚úÖ 17 chunks com elementos individuais preservados\n",
      "\n",
      "M√©dia de chunks por p√°gina: 8.5\n",
      "M√©dia de elementos individuais por chunk: 1.5\n",
      "Total de elementos individuais para highlighting: 25\n",
      "Total de p√°ginas com chunks: 2\n",
      "\n",
      "================================================================================\n",
      "üìÑ Processando arquivo: 6608-0b2b2a3568d30e31ff80e4ab897e6711.pdf\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: The PDF <_io.BufferedReader name='C:\\\\Users\\\\User\\\\Workplace\\\\not-a-rag-chat\\\\src\\\\notebooks\\\\data\\\\6608-0b2b2a3568d30e31ff80e4ab897e6711.pdf'> contains a metadata field indicating that it should not allow text extraction. Ignoring this field and proceeding. Use the check_extractable if you want to raise an error in this case\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n",
      "Number of documents loaded: 33\n",
      "‚úÖ Plots por p√°gina salvos em C:\\Users\\User\\Workplace\\not-a-rag-chat\\src\\notebooks\\outputs-text-splitter\\page_plots\\6608-0b2b2a3568d30e31ff80e4ab897e6711\n",
      "üìÑ Total de p√°ginas encontradas: 2\n",
      "‚úÖ 2 parent documents criados\n",
      "üîß Criando chunks sem√¢nticos preservando elementos individuais...\n",
      "  P√°gina 1: 16 elementos ‚Üí 9 chunks sem√¢nticos\n",
      "  P√°gina 2: 17 elementos ‚Üí 5 chunks sem√¢nticos\n",
      "\n",
      "============================================================\n",
      "‚úÖ 2 parents armazenados\n",
      "‚úÖ 14 chunks sem√¢nticos gerados\n",
      "‚úÖ 14 chunks com elementos individuais preservados\n",
      "\n",
      "M√©dia de chunks por p√°gina: 7.0\n",
      "M√©dia de elementos individuais por chunk: 2.4\n",
      "Total de elementos individuais para highlighting: 33\n",
      "Total de p√°ginas com chunks: 2\n",
      "\n",
      "================================================================================\n",
      "üìÑ Processando arquivo: 6608-0b3ec29e2bc981e9a0dcd22dee8fd74b.pdf\n",
      "================================================================================\n",
      "Warning: No languages specified, defaulting to English.\n",
      "Number of documents loaded: 20\n",
      "‚úÖ Plots por p√°gina salvos em C:\\Users\\User\\Workplace\\not-a-rag-chat\\src\\notebooks\\outputs-text-splitter\\page_plots\\6608-0b3ec29e2bc981e9a0dcd22dee8fd74b\n",
      "üìÑ Total de p√°ginas encontradas: 1\n",
      "‚úÖ 1 parent documents criados\n",
      "üîß Criando chunks sem√¢nticos preservando elementos individuais...\n",
      "  P√°gina 1: 20 elementos ‚Üí 6 chunks sem√¢nticos\n",
      "\n",
      "============================================================\n",
      "‚úÖ 1 parents armazenados\n",
      "‚úÖ 6 chunks sem√¢nticos gerados\n",
      "‚úÖ 6 chunks com elementos individuais preservados\n",
      "\n",
      "M√©dia de chunks por p√°gina: 6.0\n",
      "M√©dia de elementos individuais por chunk: 3.3\n",
      "Total de elementos individuais para highlighting: 20\n",
      "Total de p√°ginas com chunks: 1\n",
      "\n",
      "================================================================================\n",
      "üìÑ Processando arquivo: 6608-e36b4c860a90e543f5aff1c99d8057be.pdf\n",
      "================================================================================\n",
      "Warning: No languages specified, defaulting to English.\n",
      "Number of documents loaded: 57\n",
      "‚úÖ Plots por p√°gina salvos em C:\\Users\\User\\Workplace\\not-a-rag-chat\\src\\notebooks\\outputs-text-splitter\\page_plots\\6608-e36b4c860a90e543f5aff1c99d8057be\n",
      "üìÑ Total de p√°ginas encontradas: 12\n",
      "‚úÖ 12 parent documents criados\n",
      "üîß Criando chunks sem√¢nticos preservando elementos individuais...\n",
      "  P√°gina 1: 10 elementos ‚Üí 3 chunks sem√¢nticos\n",
      "  P√°gina 2: 7 elementos ‚Üí 5 chunks sem√¢nticos\n",
      "  P√°gina 3: 4 elementos ‚Üí 1 chunks sem√¢nticos\n",
      "  P√°gina 4: 4 elementos ‚Üí 1 chunks sem√¢nticos\n",
      "  P√°gina 5: 4 elementos ‚Üí 1 chunks sem√¢nticos\n",
      "  P√°gina 6: 4 elementos ‚Üí 1 chunks sem√¢nticos\n",
      "  P√°gina 7: 4 elementos ‚Üí 1 chunks sem√¢nticos\n",
      "  P√°gina 8: 4 elementos ‚Üí 1 chunks sem√¢nticos\n",
      "  P√°gina 9: 4 elementos ‚Üí 1 chunks sem√¢nticos\n",
      "  P√°gina 10: 4 elementos ‚Üí 1 chunks sem√¢nticos\n",
      "  P√°gina 11: 4 elementos ‚Üí 1 chunks sem√¢nticos\n",
      "  P√°gina 12: 4 elementos ‚Üí 1 chunks sem√¢nticos\n",
      "\n",
      "============================================================\n",
      "‚úÖ 12 parents armazenados\n",
      "‚úÖ 18 chunks sem√¢nticos gerados\n",
      "‚úÖ 18 chunks com elementos individuais preservados\n",
      "\n",
      "M√©dia de chunks por p√°gina: 1.5\n",
      "M√©dia de elementos individuais por chunk: 3.2\n",
      "Total de elementos individuais para highlighting: 57\n",
      "Total de p√°ginas com chunks: 12\n",
      "\n",
      "‚è±Ô∏è Pipeline conclu√≠do em 0:00:46.981929 (hh:mm:ss)\n",
      "\n",
      "Resumo por arquivo:\n",
      "- 6608-06e0e21ca08fc4373941c452c916f536.pdf: 52 elementos, 6 parents, 14 chunks, plots de p√°ginas: 6, plots de chunks: 6.\n",
      "  JSON: C:\\Users\\User\\Workplace\\not-a-rag-chat\\src\\notebooks\\outputs-text-splitter\\elements\\6608-06e0e21ca08fc4373941c452c916f536.json\n",
      "  Exemplos de plots de p√°gina: C:\\Users\\User\\Workplace\\not-a-rag-chat\\src\\notebooks\\outputs-text-splitter\\page_plots\\6608-06e0e21ca08fc4373941c452c916f536\n",
      "  Plots de chunks: C:\\Users\\User\\Workplace\\not-a-rag-chat\\src\\notebooks\\outputs-text-splitter\\chunk_plots\\6608-06e0e21ca08fc4373941c452c916f536\n",
      "- 6608-0a1d0058940a3f53d22f922124b0e884.pdf: 57 elementos, 7 parents, 42 chunks, plots de p√°ginas: 7, plots de chunks: 7.\n",
      "  JSON: C:\\Users\\User\\Workplace\\not-a-rag-chat\\src\\notebooks\\outputs-text-splitter\\elements\\6608-0a1d0058940a3f53d22f922124b0e884.json\n",
      "  Exemplos de plots de p√°gina: C:\\Users\\User\\Workplace\\not-a-rag-chat\\src\\notebooks\\outputs-text-splitter\\page_plots\\6608-0a1d0058940a3f53d22f922124b0e884\n",
      "  Plots de chunks: C:\\Users\\User\\Workplace\\not-a-rag-chat\\src\\notebooks\\outputs-text-splitter\\chunk_plots\\6608-0a1d0058940a3f53d22f922124b0e884\n",
      "- 6608-0a2e8a9ec5446b75ee84f3b02c5ef686.pdf: 41 elementos, 1 parents, 4 chunks, plots de p√°ginas: 1, plots de chunks: 1.\n",
      "  JSON: C:\\Users\\User\\Workplace\\not-a-rag-chat\\src\\notebooks\\outputs-text-splitter\\elements\\6608-0a2e8a9ec5446b75ee84f3b02c5ef686.json\n",
      "  Exemplos de plots de p√°gina: C:\\Users\\User\\Workplace\\not-a-rag-chat\\src\\notebooks\\outputs-text-splitter\\page_plots\\6608-0a2e8a9ec5446b75ee84f3b02c5ef686\n",
      "  Plots de chunks: C:\\Users\\User\\Workplace\\not-a-rag-chat\\src\\notebooks\\outputs-text-splitter\\chunk_plots\\6608-0a2e8a9ec5446b75ee84f3b02c5ef686\n",
      "- 6608-0a5f52f754cb4976bc8685974ac6a916.pdf: 19 elementos, 1 parents, 6 chunks, plots de p√°ginas: 1, plots de chunks: 1.\n",
      "  JSON: C:\\Users\\User\\Workplace\\not-a-rag-chat\\src\\notebooks\\outputs-text-splitter\\elements\\6608-0a5f52f754cb4976bc8685974ac6a916.json\n",
      "  Exemplos de plots de p√°gina: C:\\Users\\User\\Workplace\\not-a-rag-chat\\src\\notebooks\\outputs-text-splitter\\page_plots\\6608-0a5f52f754cb4976bc8685974ac6a916\n",
      "  Plots de chunks: C:\\Users\\User\\Workplace\\not-a-rag-chat\\src\\notebooks\\outputs-text-splitter\\chunk_plots\\6608-0a5f52f754cb4976bc8685974ac6a916\n",
      "- 6608-0a8352b90299a624fec5b492f07d38b1.pdf: 30 elementos, 1 parents, 6 chunks, plots de p√°ginas: 1, plots de chunks: 1.\n",
      "  JSON: C:\\Users\\User\\Workplace\\not-a-rag-chat\\src\\notebooks\\outputs-text-splitter\\elements\\6608-0a8352b90299a624fec5b492f07d38b1.json\n",
      "  Exemplos de plots de p√°gina: C:\\Users\\User\\Workplace\\not-a-rag-chat\\src\\notebooks\\outputs-text-splitter\\page_plots\\6608-0a8352b90299a624fec5b492f07d38b1\n",
      "  Plots de chunks: C:\\Users\\User\\Workplace\\not-a-rag-chat\\src\\notebooks\\outputs-text-splitter\\chunk_plots\\6608-0a8352b90299a624fec5b492f07d38b1\n",
      "- 6608-0af9a89a5bd79cb8becd09af9eaeeac0.PDF: 25 elementos, 2 parents, 17 chunks, plots de p√°ginas: 2, plots de chunks: 2.\n",
      "  JSON: C:\\Users\\User\\Workplace\\not-a-rag-chat\\src\\notebooks\\outputs-text-splitter\\elements\\6608-0af9a89a5bd79cb8becd09af9eaeeac0.json\n",
      "  Exemplos de plots de p√°gina: C:\\Users\\User\\Workplace\\not-a-rag-chat\\src\\notebooks\\outputs-text-splitter\\page_plots\\6608-0af9a89a5bd79cb8becd09af9eaeeac0\n",
      "  Plots de chunks: C:\\Users\\User\\Workplace\\not-a-rag-chat\\src\\notebooks\\outputs-text-splitter\\chunk_plots\\6608-0af9a89a5bd79cb8becd09af9eaeeac0\n",
      "- 6608-0b2b2a3568d30e31ff80e4ab897e6711.pdf: 33 elementos, 2 parents, 14 chunks, plots de p√°ginas: 2, plots de chunks: 2.\n",
      "  JSON: C:\\Users\\User\\Workplace\\not-a-rag-chat\\src\\notebooks\\outputs-text-splitter\\elements\\6608-0b2b2a3568d30e31ff80e4ab897e6711.json\n",
      "  Exemplos de plots de p√°gina: C:\\Users\\User\\Workplace\\not-a-rag-chat\\src\\notebooks\\outputs-text-splitter\\page_plots\\6608-0b2b2a3568d30e31ff80e4ab897e6711\n",
      "  Plots de chunks: C:\\Users\\User\\Workplace\\not-a-rag-chat\\src\\notebooks\\outputs-text-splitter\\chunk_plots\\6608-0b2b2a3568d30e31ff80e4ab897e6711\n",
      "- 6608-0b3ec29e2bc981e9a0dcd22dee8fd74b.pdf: 20 elementos, 1 parents, 6 chunks, plots de p√°ginas: 1, plots de chunks: 1.\n",
      "  JSON: C:\\Users\\User\\Workplace\\not-a-rag-chat\\src\\notebooks\\outputs-text-splitter\\elements\\6608-0b3ec29e2bc981e9a0dcd22dee8fd74b.json\n",
      "  Exemplos de plots de p√°gina: C:\\Users\\User\\Workplace\\not-a-rag-chat\\src\\notebooks\\outputs-text-splitter\\page_plots\\6608-0b3ec29e2bc981e9a0dcd22dee8fd74b\n",
      "  Plots de chunks: C:\\Users\\User\\Workplace\\not-a-rag-chat\\src\\notebooks\\outputs-text-splitter\\chunk_plots\\6608-0b3ec29e2bc981e9a0dcd22dee8fd74b\n",
      "- 6608-e36b4c860a90e543f5aff1c99d8057be.pdf: 57 elementos, 12 parents, 18 chunks, plots de p√°ginas: 12, plots de chunks: 12.\n",
      "  JSON: C:\\Users\\User\\Workplace\\not-a-rag-chat\\src\\notebooks\\outputs-text-splitter\\elements\\6608-e36b4c860a90e543f5aff1c99d8057be.json\n",
      "  Exemplos de plots de p√°gina: C:\\Users\\User\\Workplace\\not-a-rag-chat\\src\\notebooks\\outputs-text-splitter\\page_plots\\6608-e36b4c860a90e543f5aff1c99d8057be\n",
      "  Plots de chunks: C:\\Users\\User\\Workplace\\not-a-rag-chat\\src\\notebooks\\outputs-text-splitter\\chunk_plots\\6608-e36b4c860a90e543f5aff1c99d8057be\n"
     ]
    }
   ],
   "source": [
    "import traceback\n",
    "from datetime import datetime\n",
    "\n",
    "processing_results = []\n",
    "start_time = datetime.now()\n",
    "\n",
    "for pdf_path in PDF_PATHS:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üìÑ Processando arquivo: {pdf_path.name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    try:\n",
    "        loader = UnstructuredLoader(str(pdf_path), partition_kwargs={\"languages\": [\"por\"], \"strategy\": \"hi_res\"})\n",
    "        elements = loader.load()\n",
    "        print(f\"Number of documents loaded: {len(elements)}\")\n",
    "\n",
    "        for elem in elements:\n",
    "            elem.metadata.setdefault('filename', pdf_path.name)\n",
    "            elem.metadata.setdefault('source', str(pdf_path))\n",
    "            elem.metadata['source_path'] = str(pdf_path)\n",
    "\n",
    "        serialized_path = ELEMENTS_OUTPUT_DIR / f\"{pdf_path.stem}.json\"\n",
    "        dump_elements(elements, serialized_path)\n",
    "        page_stats = highlight_elements(pdf_path, elements, PAGE_PLOTS_DIR)\n",
    "        parent_documents, pages_dict = build_parent_documents(elements, pdf_path)\n",
    "        semantic_payload = build_semantic_children(parent_documents, pages_dict, pdf_path)\n",
    "\n",
    "        chunks_output_dir = ELEMENTS_OUTPUT_DIR / \"chunks\"\n",
    "        chunks_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        chunks_json_path = chunks_output_dir / f\"{pdf_path.stem}_chunks.json\"\n",
    "\n",
    "        chunk_records = []\n",
    "        for child_id, chunk_info in semantic_payload['elements_with_coords'].items():\n",
    "            chunk_records.append({\n",
    "                \"chunk_id\": child_id,\n",
    "                \"text\": chunk_info['content'],\n",
    "                \"category\": chunk_info['category'],\n",
    "                \"page_number\": chunk_info['page_number'],\n",
    "                \"source_elements_count\": chunk_info['source_elements_count'],\n",
    "                \"chunk_position\": chunk_info['chunk_position'],\n",
    "                \"contributing_elements\": [\n",
    "                    {\n",
    "                        \"coordinates\": elem_data['coordinates'],\n",
    "                        \"category\": elem_data['category'],\n",
    "                        \"content\": elem_data['content'],\n",
    "                        \"overlap_percentage\": elem_data['overlap_percentage']\n",
    "                    }\n",
    "                    for elem_data in chunk_info['contributing_elements']\n",
    "                ]\n",
    "            })\n",
    "\n",
    "        with open(chunks_json_path, \"w\", encoding=\"utf-8\") as fp:\n",
    "            json.dump(chunk_records, fp, ensure_ascii=False, indent=2)\n",
    "        print(f\"üíæ Registro dos chunks salvo em {chunks_json_path}\")\n",
    "\n",
    "        chunk_plots_dir = highlight_chunks(pdf_path, semantic_payload['elements_with_coords'], CHUNK_PLOTS_DIR)\n",
    "\n",
    "        chunk_pages = {info['page_number'] for info in semantic_payload['elements_with_coords'].values()}\n",
    "        processing_results.append({\n",
    "            'pdf': pdf_path,\n",
    "            'elements_path': serialized_path,\n",
    "            'chunks_path': chunks_json_path,\n",
    "            'page_plots': page_stats,\n",
    "            'chunk_plots_dir': chunk_plots_dir,\n",
    "            'total_elements': len(elements),\n",
    "            'parent_documents': len(parent_documents),\n",
    "            'semantic_children': len(semantic_payload['semantic_children']),\n",
    "            'chunk_pages': len(chunk_pages)\n",
    "        })\n",
    "    except Exception as exc:\n",
    "        print(f\"‚ö†Ô∏è Erro ao processar {pdf_path.name}: {exc}\")\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "elapsed = datetime.now() - start_time\n",
    "print(f\"\\n‚è±Ô∏è Pipeline conclu√≠do em {elapsed} (hh:mm:ss)\")\n",
    "\n",
    "if processing_results:\n",
    "    print(\"\\nResumo por arquivo:\")\n",
    "    for result in processing_results:\n",
    "        page_plot_count = len(result['page_plots']) if result['page_plots'] else 0\n",
    "        chunk_plot_path = str(result['chunk_plots_dir']) if result['chunk_plots_dir'] else 'N/A'\n",
    "        summary_line = \"- {name}: {elements} elementos, {parents} parents, {chunks} chunks, plots de p√°ginas: {pages}, plots de chunks: {chunk_pages}.\".format(\n",
    "            name=result['pdf'].name,\n",
    "            elements=result['total_elements'],\n",
    "            parents=result['parent_documents'],\n",
    "            chunks=result['semantic_children'],\n",
    "            pages=page_plot_count,\n",
    "            chunk_pages=result['chunk_pages']\n",
    "        )\n",
    "        print(summary_line)\n",
    "        print(f\"  JSON de elementos: {result['elements_path']}\")\n",
    "        print(f\"  JSON de chunks: {result['chunks_path']}\")\n",
    "        if page_plot_count:\n",
    "            first_page_plot = result['page_plots'][0][2]\n",
    "            print(f\"  Exemplos de plots de p√°gina: {first_page_plot.parent}\")\n",
    "        print(f\"  Plots de chunks: {chunk_plot_path}\")\n",
    "else:\n",
    "    print(\"Nenhum arquivo foi processado.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "not-a-rag-chat (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
